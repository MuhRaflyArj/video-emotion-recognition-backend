{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "054395a9",
   "metadata": {},
   "source": [
    "<h1 align=center> üìπ End-to-End Video Emotion Recognition using MediaPipe and a CNN-LSTM Architecture ü§ñ </h1>\n",
    "\n",
    "This project details a complete system for recognizing human emotions from short video clips, designed for a journaling application üìì that uses video as a cover for entries. \n",
    "\n",
    "A key innovation of this notebook is its unique data processing pipeline. It starts with the AFEW-VA dataset üìä, which only provides continuous `valence` and `arousal` scores. This pipeline transforms those scores into discrete emotion labels by:\n",
    "1.  Averaging the scores per video.\n",
    "2.  Applying K-Means clustering üß© to group the data.\n",
    "3.  Performing manual curation ‚úçÔ∏è to ensure high-quality labels.\n",
    "\n",
    "The system then uses MediaPipe üåê to extract 3D face mesh vectors from video frames and feeds sequences of this data into a hybrid **CNN-LSTM model** üß† to classify the final emotion. üòä\n",
    "\n",
    "The resulting five emotion labels used for training are: \n",
    "- Anger\n",
    "- Happy \n",
    "- Shock\n",
    "- Neutral\n",
    "- Sad\n",
    "\n",
    "## üí° Use Case Possibilities\n",
    "\n",
    "While this model was initially designed for a journaling app, the core technology can be adapted for a wide range of powerful applications:\n",
    "\n",
    "* **üìì Enhanced Digital Journaling & Mental Wellness:** The primary use case. The app can track mood patterns over time, helping users gain insight into their emotional wellbeing. It could suggest activities or resources based on detected emotional trends.\n",
    "\n",
    "* **üß† User Experience (UX) Research:** Companies can analyze user reactions to new software, websites, or products in real-time. This provides authentic, unbiased feedback on whether a feature is delightful, confusing, or frustrating.\n",
    "\n",
    "* **üìö Adaptive E-Learning Platforms:** An online learning system could gauge a student's emotional state. If the system detects confusion or frustration, it could automatically offer hints, supplementary materials, or a different teaching approach.\n",
    "\n",
    "* **üé¨ Audience Reaction Analysis:** Media companies could use this to analyze audience reactions during movie screenings or trailer tests to gauge emotional engagement with the content.\n",
    "\n",
    "* **üöó Driver Monitoring Systems:** In-car cameras could use the model to detect driver states like drowsiness, distraction, or road rage, triggering safety alerts to prevent accidents.\n",
    "\n",
    "<video width=\"600\" controls>\n",
    "  <source src=\"./Assets/journaling_illustration.mp4\" type=\"video/mp4\">\n",
    "</video>\n",
    "\n",
    "*The illustration is generated by AI\n",
    "\n",
    "### üìà Key Results\n",
    "* The final CNN-LSTM model achieved a **validation accuracy of 79.52%** on the curated test set.\n",
    "* Successfully demonstrated a full pipeline from raw data with continuous labels to a functioning discrete emotion classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04abd6a",
   "metadata": {},
   "source": [
    "### üìä About the AFEW-VA Dataset\n",
    "\n",
    "*Dataset Source:\n",
    "```\n",
    "J. Kossaifi, G. Tzimiropoulos, S. Todorovic, and M. Pantic, \"AFEW-VA database for valence and arousal estimation in-the-wild,\" Image and Vision Computing, vol. 65, pp. 23-36, 2017, doi: 10.1016/j.imavis.2017.02.001.\n",
    "```\n",
    "\n",
    "![AFEW-VA Valence-Arousal Distribution](./Assets/afew-va_example.png)\n",
    "\n",
    "#### **Key Characteristics**\n",
    "* **üé¨ Content**: 600 video clips from feature films, selected for their challenging nature.\n",
    "* **üèûÔ∏è Environment**: \"In-the-wild\" conditions with complex backgrounds, lighting, and head poses.\n",
    "* **üè∑Ô∏è Annotations**: Provides two types of labels for each frame:\n",
    "    * `valence` (positive/negative) and `arousal` (calm/excited) scores on a -10 to 10 scale.\n",
    "    * The locations of 68 facial landmarks.\n",
    "* **‚úçÔ∏è Annotation Method**: A custom-built online tool was used for precise per-frame labeling by two collaborating, FACS-certified experts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a97ed21",
   "metadata": {},
   "source": [
    "### ‚öôÔ∏è 1. Preparing the Dataset\n",
    "\n",
    "The following code handles the initial data setup. We import the required libraries, download the AFEW-VA dataset from Kaggle Hub, and then use a parallelized script to efficiently copy the files into our local `./Dataset` folder for the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1e8b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "import os\n",
    "import shutil\n",
    "import concurrent.futures\n",
    "from functools import partial\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29042d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_item(item, source_dir, dest_dir):\n",
    "    \"\"\"\n",
    "    Helper function to copy a single item from source to destination\n",
    "    \"\"\"\n",
    "    src_path = os.path.join(source_dir, item)\n",
    "    dst_path = os.path.join(dest_dir, item)\n",
    "    \n",
    "    # If the destination path exists, replace it\n",
    "    if os.path.exists(dst_path):\n",
    "        if os.path.isdir(dst_path):\n",
    "            shutil.rmtree(dst_path)\n",
    "        else:\n",
    "            os.remove(dst_path)\n",
    "            \n",
    "    if os.path.isdir(src_path):\n",
    "        shutil.copytree(src_path, dst_path)\n",
    "    else:\n",
    "        shutil.copy2(src_path, dst_path)\n",
    "\n",
    "destination_dir = './Dataset/AFEW-VA'\n",
    "os.makedirs(destination_dir, exist_ok=True)\n",
    "\n",
    "# Download data from kaggle\n",
    "cache_path = kagglehub.dataset_download(\"hoanguyensgu/afew-va\")\n",
    "source_dir = os.path.join(cache_path, \"AFEW-VA\")\n",
    "items = os.listdir(source_dir)\n",
    "\n",
    "# Prepare parallel copy task\n",
    "copy_task = partial(copy_item, source_dir=source_dir, dest_dir=destination_dir)\n",
    "num_workers = os.cpu_count() * 2\n",
    "\n",
    "# Copy data in parallel\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "    list(tqdm(executor.map(copy_task, items), total=len(items), desc='Copying Files'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa1c01b",
   "metadata": {},
   "source": [
    "## üî¨ 2. Data Processing & Feature Extraction\n",
    "\n",
    "![Feature Extraction Pipeline](./Assets/feature_extraction.png)\n",
    "\n",
    "This is the core data engineering section of the project. The goal here is to transform the raw AFEW-VA dataset into a structured, feature-rich format that is ready for training our deep learning model. This involves a multi-step pipeline:\n",
    "\n",
    "1.  **üé¨ Video Generation**: The initial dataset provides videos as sequences of images. The first step is to convert these image sequences into standard `.mp4` video files for easier processing.\n",
    "\n",
    "2.  **üè∑Ô∏è Emotion Label Generation**: Since the dataset provides continuous `valence` and `arousal` scores instead of discrete emotions, we generate our own labels. This is done by:\n",
    "    * Averaging the valence and arousal scores for each video.\n",
    "    * Using **K-Means clustering** on these averages to group videos into distinct emotional categories.\n",
    "    * Assigning human-readable labels (`Anger`, `Happy`, etc.) to these clusters.\n",
    "    * Performing a final manual review and correction using an external CSV file to ensure label accuracy. The videos are then reorganized into folders named after their final emotion label.\n",
    "\n",
    "3.  **üåê Face Mesh Extraction with MediaPipe**: Each labeled video is processed frame-by-frame using Google's MediaPipe Face Mesh solution. This powerful tool extracts the 3D coordinates (x, y, z) for **468 distinct facial landmarks** for every face detected.\n",
    "\n",
    "4.  **üìú CSV Export for Sequencing**: The extracted landmark data is exported into CSV files. To prepare the data for the CNN-LSTM model, the landmarks from each video are chunked into sequences of **15 frames**. Each resulting CSV file represents one of these 15-frame sequences, which will become a single input sample for our model.\n",
    "\n",
    "**The entire pipeline is divided into three main stages**, each executed by the code cells below:\n",
    "\n",
    "* üé¨ **Stage 1: Video Generation** - Converts the dataset's raw image sequences into standard `.mp4` video files.\n",
    "* üè∑Ô∏è **Stage 2: Label Generation** - Creates discrete emotion labels (e.g., Happy, Sad) from the dataset's raw valence/arousal scores using K-Means clustering and manual review.\n",
    "* üìú **Stage 3: Feature Extraction** - Processes the labeled videos with MediaPipe to extract 3D facial landmarks and saves them as sequential 15-frame CSV files, preparing the data for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9cb24fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import csv\n",
    "import shutil\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "import tqdm\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96085402",
   "metadata": {},
   "source": [
    "### üé¨ **Stage 1: Converting Image Sequences to MP4**\n",
    "\n",
    "The first stage of the pipeline focuses on standardizing the data format. The original AFEW-VA dataset provides each video as a folder containing a sequence of PNG images. This code iterates through each of these folders, reads the sorted image sequence, and uses OpenCV to compile them into a single, standard `.mp4` video file. This process is parallelized using a thread pool to efficiently handle all 600 videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eac6f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_video_from_images(image_folder, output_video_path, fps=30):\n",
    "    \"\"\"\n",
    "    Create a video from a sequence of PNG images in a folder.\n",
    "    \"\"\"\n",
    "    # List all images in a folder\n",
    "    image_files = sorted(glob.glob(os.path.join(image_folder, '*.png')))\n",
    "\n",
    "    first_frame = cv2.imread(image_files[0])\n",
    "    height, width, layers = first_frame.shape\n",
    "    frame_size = (width, height)\n",
    "\n",
    "    # Video Writer with MP4V codec\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    video_writer = cv2.VideoWriter(output_video_path, fourcc, fps, frame_size)\n",
    "\n",
    "    for image_file in image_files:\n",
    "        frame = cv2.imread(image_file)\n",
    "        video_writer.write(frame)\n",
    "\n",
    "    video_writer.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484bea4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_valence_arousal(base_folder):\n",
    "    \"\"\"\n",
    "    Calculate each video and overall valence and arousal statistics.\n",
    "    \"\"\"\n",
    "    per_file_average = {}\n",
    "\n",
    "    overall_min_valence = float('inf')\n",
    "    overall_max_valence = float('-inf')\n",
    "    overall_min_arousal = float('inf')\n",
    "    overall_max_arousal = float('-inf')\n",
    "\n",
    "    for folder_name in sorted(os.listdir(base_folder)):\n",
    "        folder_path = os.path.join(base_folder, folder_name)\n",
    "        if os.path.isdir(folder_path):\n",
    "            json_path = os.path.join(folder_path, f\"{folder_name}.json\")\n",
    "            with open(json_path, 'r', encoding='utf-8') as file:\n",
    "                data = json.load(file)\n",
    "\n",
    "            frames = data['frames']\n",
    "\n",
    "            valences = [frame_data['valence'] for frame_data in frames.values()]\n",
    "            arousals = [frame_data['arousal'] for frame_data in frames.values()]\n",
    "\n",
    "            min_valence, max_valence = min(valences), max(valences)\n",
    "            avg_valence = sum(valences) / len(valences)\n",
    "            \n",
    "            min_arousal, max_arousal = min(arousals), max(arousals)\n",
    "            avg_arousal = sum(arousals) / len(arousals)\n",
    "\n",
    "            # Statistics for each video\n",
    "            per_file_average[folder_name] = {\n",
    "                \"average_valence\": avg_valence,\n",
    "                \"min_valence\": min_valence,\n",
    "                \"max_valence\": max_valence,\n",
    "                \"average_arousal\": avg_arousal,\n",
    "                \"min_arousal\": min_arousal,\n",
    "                \"max_arousal\": max_arousal,\n",
    "            }\n",
    "            \n",
    "            overall_min_valence = min(overall_min_valence, min_valence)\n",
    "            overall_max_valence = max(overall_max_valence, max_valence)\n",
    "            overall_min_arousal = min(overall_min_arousal, min_arousal)\n",
    "            overall_max_arousal = max(overall_max_arousal, max_arousal)\n",
    "\n",
    "        final_data = {\n",
    "            \"per_file_average\": per_file_average,\n",
    "            \"overall_stats\": {\n",
    "                \"overall_min_valence\": overall_min_valence,\n",
    "                \"overall_max_valence\": overall_max_valence,\n",
    "                \"overall_min_arousal\": overall_min_arousal,\n",
    "                \"overall_max_arousal\": overall_max_arousal,\n",
    "            }\n",
    "        }\n",
    "\n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469c8006",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = './Dataset/AFEW-VA'\n",
    "video_output_folder = './Data/AFEW-VA/Video'\n",
    "facemesh_output_folder = './Data/AFEW-VA/FaceMesh'\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs(video_output_folder, exist_ok=True)\n",
    "os.makedirs(facemesh_output_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c833e340",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def process_folder(folder_path):\n",
    "    folder_name = os.path.basename(folder_path)\n",
    "    output_video_file = os.path.join(video_output_folder, f'{folder_name}.mp4')\n",
    "    if os.path.isdir(folder_path):\n",
    "        create_video_from_images(folder_path, output_video_file, fps=30)\n",
    "\n",
    "subfolders = sorted([f.path for f in os.scandir(base_folder)])\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=os.cpu_count() * 2) as executor:\n",
    "    list(tqdm.tqdm(executor.map(process_folder, subfolders), total=len(subfolders), desc='Generating Video'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6c8e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get average valence and arousal for each video\n",
    "analysis_results = average_valence_arousal(base_folder)\n",
    "per_file_data = analysis_results.get(\"per_file_average\", {})\n",
    "\n",
    "# Collect average valence and arousal from each video\n",
    "avg_valences = [data['average_valence'] for data in per_file_data.values()]\n",
    "avg_arousals = [data['average_arousal'] for data in per_file_data.values()]\n",
    "\n",
    "# Plotting the distribution of valence and arousal\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(\n",
    "    avg_valences, avg_arousals,\n",
    "    alpha=0.8, edgecolors='k', s=80, c=avg_arousals, cmap='coolwarm'\n",
    ")\n",
    "\n",
    "plt.title('Valence vs. Arousal Distribution', fontsize=18, fontweight='bold')\n",
    "plt.xlabel('Valence (Positive/Negative)', fontsize=14)\n",
    "plt.ylabel('Arousal (Calm/Excited)', fontsize=14)\n",
    "\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.axhline(0, color='grey', lw=1.2, linestyle='--')\n",
    "plt.axvline(0, color='grey', lw=1.2, linestyle='--')\n",
    "\n",
    "plt.colorbar(label='Arousal Value')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29000242",
   "metadata": {},
   "source": [
    "#### üî¨ Analyzing the Valence-Arousal Space\n",
    "\n",
    "This scatter plot visualizes the emotional landscape of the entire AFEW-VA dataset. Each point on the chart represents one of the 600 videos, positioned based on its average `valence` (the horizontal axis, representing negative to positive sentiment) and `arousal` (the vertical axis, representing calm to excited energy levels).\n",
    "\n",
    "**Justification for Clustering**\n",
    "\n",
    "The key insight from this plot is that the data points are not randomly scattered. Instead, they form a distinct V-shape structure, which is characteristic of emotional distributions. We can clearly see several natural groupings or \"clouds\" of data points:\n",
    "\n",
    "* **Top-Left Quadrant**: A dense cluster with high arousal and negative valence, likely corresponding to emotions like **Anger** or **Fear**.\n",
    "* **Top-Right Quadrant**: Another dense cluster with high arousal and positive valence, suggesting emotions like **Happy** or **Excited**.\n",
    "* **Lower Regions**: A concentration of points with low arousal, which could represent **Sad** or **Neutral** states.\n",
    "\n",
    "Because these visually separable groups exist, it provides a strong justification for using a clustering algorithm. The goal of **K-Means clustering** in the next step is to mathematically identify the centers of these natural groups. We can then assign a discrete emotion label (e.g., `Happy`, `Sad`) to each cluster, effectively transforming the continuous valence-arousal data into the distinct classes needed to train our classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6019d26e",
   "metadata": {},
   "source": [
    "### üè∑Ô∏è **Stage 2: Emotion Label Generation via Clustering**\n",
    "\n",
    "![Clustering Illustration](./Assets/clustering.png)\n",
    "\n",
    "This stage addresses the core challenge of the AFEW-VA dataset: the absence of discrete emotion labels. The process to generate high-quality labels is as follows:\n",
    "1.  **Aggregate Scores**: The continuous `valence` and `arousal` scores for every frame in a video are averaged into a single representative point.\n",
    "2.  **Explore Clusters**: K-Means clustering is performed with various cluster counts (k=3 to 10) to find the most natural grouping of the data.\n",
    "3.  **Assign Labels**: Based on visual inspection, **k=5** is chosen as the optimal number of clusters. These clusters are then mapped to the five emotion labels: `Anger`, `Happy`, `Shock`, `Neutral`, and `Sad`. The mapping is carefully done by sorting the cluster centroids to ensure consistent labeling.\n",
    "4.  **Organize & Correct**: The `.mp4` videos generated in Stage 1 are moved into subfolders named after their newly assigned emotion label. A final manual correction step is performed by reading a `label_correction.csv` file to move any misclassified videos, ensuring the final labeled dataset is as accurate as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a03879",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_clustering(X, num_clusters):\n",
    "    \"\"\"\n",
    "    Get centroid coordinate and labels on each point with KMeans clustering.\n",
    "    \"\"\"\n",
    "    kmeans = KMeans(n_clusters=num_clusters, random_state=42, n_init=10)\n",
    "    kmeans.fit(X)\n",
    "\n",
    "    labels = kmeans.labels_ # Cluster assignments\n",
    "    centers = kmeans.cluster_centers_ # Cluster centroids\n",
    "\n",
    "    return labels, centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7e4360",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_labels(base_folder, num_clusters, emotion_labels, output_filename):\n",
    "    \"\"\"\n",
    "    Assign emotion labels to each video using KMeans clustering and save as JSON\n",
    "    \"\"\"\n",
    "    analysis_results = average_valence_arousal(base_folder)\n",
    "    per_file_data = analysis_results.get(\"per_file_average\", {})\n",
    "    folder_names = list(per_file_data.keys())\n",
    "\n",
    "    # Prepare data for clustering\n",
    "    X = np.array([[data['average_valence'], data['average_arousal']] for data in per_file_data.values()])\n",
    "    cluster_indices, centers = kmeans_clustering(X, num_clusters)\n",
    "    \n",
    "    # Sort cluster centers by descending arousal and then by valence\n",
    "    sorted_centroid_indices = sorted(range(num_clusters), key=lambda k: (-centers[k][1], centers[k][0]))\n",
    "    cluster_index_to_label = {\n",
    "        original_index: emotion_labels[i]\n",
    "        for i, original_index in enumerate(sorted_centroid_indices)\n",
    "    }\n",
    "\n",
    "    # Assign emotion labels to each video\n",
    "    for i, folder_name in enumerate(folder_names):\n",
    "        cluster_index = int(cluster_indices[i])\n",
    "        emotion_label = cluster_index_to_label[cluster_index]\n",
    "        per_file_data[folder_name]['emotion_label'] = emotion_label\n",
    "\n",
    "    # Save labeled data to JSON file\n",
    "    with open(output_filename, 'w') as file:\n",
    "        json.dump(per_file_data, file, indent=2)\n",
    "\n",
    "    return per_file_data, centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624405c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_video_to_labeled_folders(json_label_path, video_folder, destination_folder):\n",
    "    \"\"\"\n",
    "    Move video to subfolders based on emotion labels from a JSON file.\n",
    "    \"\"\"\n",
    "    with open(json_label_path, 'r') as file:\n",
    "        labeled_data = json.load(file)\n",
    "\n",
    "    for folder_name, data in labeled_data.items():\n",
    "        emotion_label = data.get('emotion_label')\n",
    "\n",
    "        source_video_path = os.path.join(video_folder, f'{folder_name}.mp4')\n",
    "        destination_subfolder = os.path.join(destination_folder, emotion_label)\n",
    "        \n",
    "        destination_path = os.path.join(destination_subfolder, f'{folder_name}.mp4')\n",
    "\n",
    "        os.makedirs(destination_subfolder, exist_ok=True)\n",
    "        shutil.move(source_video_path, destination_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d947cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frame_counts(video_folder, emotion_labels):\n",
    "    \"\"\"\n",
    "    Count the number of frames in each video for every emotion category\n",
    "    \"\"\"\n",
    "    \n",
    "    frame_counts = {emotion: [] for emotion in emotion_labels}\n",
    "    total_frame_counts = {emotion: 0 for emotion in emotion_labels}\n",
    "    \n",
    "    for emotion in emotion_labels:\n",
    "        emotion_path = os.path.join(video_folder, emotion)\n",
    "        \n",
    "        for video_file in os.listdir(emotion_path):\n",
    "            video_path = os.path.join(emotion_path, video_file)\n",
    "            \n",
    "            cap = cv2.VideoCapture(video_path)\n",
    "            \n",
    "            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "            # Total frames in the video\n",
    "            frame_counts[emotion].append(total_frames)\n",
    "            # Add to total frame count to each emotion\n",
    "            total_frame_counts[emotion] += total_frames\n",
    "            \n",
    "            cap.release()\n",
    "            \n",
    "    return frame_counts, total_frame_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd692bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_videos_by_correction(video_folder, csv_path):\n",
    "    \"\"\"\n",
    "    Move videos to subfolders based on emotion labels correction from a CSV file.\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(csv_path, 'r', newline='') as csv_file:\n",
    "        reader = csv.DictReader(csv_file)\n",
    "        \n",
    "        for row in reader:\n",
    "            file_name = row['file_name']\n",
    "            initial_label = row['initial_label']\n",
    "            corrected_label = row['corrected_label']\n",
    "            \n",
    "            source_path = os.path.join(video_folder, initial_label, file_name)\n",
    "            destination_path = os.path.join(video_folder, corrected_label, file_name)\n",
    "            \n",
    "            os.makedirs(os.path.dirname(destination_path), exist_ok=True)\n",
    "            shutil.move(source_path, destination_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abcc09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform KMeans clustering on the average valence and arousal data\n",
    "per_file_data = analysis_results.get(\"per_file_average\", {})\n",
    "X = np.array([[data['average_valence'], data['average_arousal']] for data in per_file_data.values()])\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(24, 12))\n",
    "axes = axes.flatten()\n",
    "cluster_range = range(3, 11) # Number of clusters to test\n",
    "\n",
    "for i, num_clusters in enumerate(cluster_range):\n",
    "    labels, centers = kmeans_clustering(X, num_clusters)\n",
    "    \n",
    "    ax = axes[i]\n",
    "    scatter = ax.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', alpha=0.5, s=60)\n",
    "    ax.scatter(centers[:, 0], centers[:, 1], c='red', s=150, marker='o', label='Centroids')\n",
    "\n",
    "    ax.set_title(f'Valence vs Arousal Clustering ({num_clusters} Clusters)')\n",
    "    ax.set_xlabel('Valence')\n",
    "    ax.set_ylabel('Arousal')\n",
    "\n",
    "    ax.grid(True, linestyle='--', alpha=0.6)\n",
    "    ax.axhline(0, color='grey', lw=1)\n",
    "    ax.axvline(0, color='grey', lw=1)\n",
    "\n",
    "    ax.legend\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49a06a3",
   "metadata": {},
   "source": [
    "#### üîé Determining the Optimal Number of Clusters\n",
    "\n",
    "To transform the continuous valence-arousal data into discrete emotion classes, we first need to determine the most natural number of emotion groups present in the dataset. The plots above show the result of running the K-Means clustering algorithm with a range of different cluster counts (`k`), from 3 to 10.\n",
    "\n",
    "By visually inspecting these results, we can find the \"sweet spot\" that best represents distinct, meaningful emotional categories.\n",
    "\n",
    "**Why 5 Clusters is the Sweet Spot**\n",
    "\n",
    "* **When `k` is too low (e.g., 3 or 4):** The clustering is **too general**. It groups together emotions that are clearly distinct. For example, in the `k=3` plot, all high-arousal emotions (like 'Happy', 'Anger', and 'Shock') are forced into just two groups, losing valuable nuance.\n",
    "\n",
    "* **When `k` is too high (e.g., 6 to 10):** The algorithm starts to **overfit** the data. It begins partitioning the large, natural emotional clouds into smaller, less meaningful sub-regions that don't correspond to a clear, single emotional state.\n",
    "\n",
    "* **When `k=5`:** This configuration provides the best balance. It successfully identifies the five most prominent and well-separated emotional groups that align with our intuitive understanding of emotion:\n",
    "    1.  A high-arousal, positive-valence group (**Happy**).\n",
    "    2.  A high-arousal, negative-valence group (**Anger**).\n",
    "    3.  A high-arousal, neutral-valence group (**Shock**).\n",
    "    4.  A low-arousal, negative-valence group (**Sad**).\n",
    "    5.  A low-arousal, mid-valence group (**Neutral**).\n",
    "\n",
    "Therefore, **k=5** is chosen as the optimal number of clusters. This allows us to create a rich yet manageable set of emotion labels for training our final classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce19914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the number of best clusters for KMeans\n",
    "choosen_k = 5\n",
    "# Emotion labels for the clusters\n",
    "emotion_labels = [\n",
    "    'Anger',\n",
    "    'Happy',\n",
    "    'Shock',\n",
    "    'Neutral',\n",
    "    'Sad'\n",
    "]\n",
    "\n",
    "# Write labels to JSON file with KMeans clustering results\n",
    "labeled_data, centers = write_labels(\n",
    "    base_folder,\n",
    "    choosen_k,\n",
    "    emotion_labels,\n",
    "    output_filename='./Data/AFEW-VA/labeled_emotions.json'\n",
    ")\n",
    "\n",
    "# Prepare data for plotting from the returned dictionary\n",
    "X = np.array([[data['average_valence'], data['average_arousal']] for data in labeled_data.values()])\n",
    "\n",
    "# Create a numeric label for each point for coloring the plot\n",
    "unique_labels = sorted(list(set(emotion_labels)))\n",
    "label_to_color_index = {label: i for i, label in enumerate(unique_labels)}\n",
    "plot_colors = [label_to_color_index[data['emotion_label']] for data in labeled_data.values()]\n",
    "\n",
    "# Create the verification plot\n",
    "plt.figure(figsize=(14, 12))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=plot_colors, cmap='viridis', alpha=0.6, s=80)\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='red', s=250, marker='X', label='Centroids')\n",
    "\n",
    "# Annotate each centroid with its assigned emotion label\n",
    "sorted_centroid_indices = sorted(range(choosen_k), key=lambda k: (-centers[k][1], centers[k][0]))\n",
    "for i, original_index in enumerate(sorted_centroid_indices):\n",
    "    center_coords = centers[original_index]\n",
    "    label_text = emotion_labels[i]\n",
    "    \n",
    "    plt.text(\n",
    "        center_coords[0] + 0.05, center_coords[1] + 0.05, label_text, \n",
    "        fontsize=12, \n",
    "        fontweight='bold',\n",
    "        color='black',\n",
    "        bbox=dict(\n",
    "            facecolor='white', \n",
    "            alpha=0.7, \n",
    "            edgecolor='none', \n",
    "            boxstyle='round,pad=0.2'\n",
    "        )\n",
    "    )\n",
    "\n",
    "plt.title(f'Verification Plot with Emotion Labels ({choosen_k} Clusters)', fontsize=20)\n",
    "plt.xlabel('Valence (Positive/Negative)', fontsize=14)\n",
    "plt.ylabel('Arousal (Calm/Excited)', fontsize=14)\n",
    "\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.axhline(0, color='grey', lw=1.5)\n",
    "plt.axvline(0, color='grey', lw=1.5)\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b762cc8",
   "metadata": {},
   "source": [
    "#### ‚úÖ Final Label Verification\n",
    "\n",
    "This verification plot shows the final result of our data labeling process. After confirming that 5 clusters provide the best separation, a specific emotion label has been assigned to each cluster. Each data point (representing a video) is now colored according to its final assigned emotion, and the cluster centers (centroids) are marked with a red 'X'.\n",
    "\n",
    "The emotion labels were assigned logically based on each centroid's position within the two-dimensional emotional space:\n",
    "\n",
    "* **Anger**: Located in the high-arousal, negative-valence quadrant.\n",
    "* **Happy**: Located in the high-arousal, positive-valence quadrant.\n",
    "* **Shock**: Positioned in the high-arousal, near-neutral valence area, representing a state of surprise.\n",
    "* **Sad**: Found in the low-arousal, negative-valence quadrant.\n",
    "* **Neutral**: Positioned near the origin, representing a state of low arousal and neutral valence.\n",
    "\n",
    "This plot visually confirms that our pipeline has successfully transformed the raw, continuous valence-arousal data into a well-defined and intuitively correct set of five discrete emotion categories. With this labeled and verified dataset, we are now ready to proceed to the feature extraction stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7802a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "move_video_to_labeled_folders(\n",
    "    './Data/AFEW-VA/labeled_emotions.json',\n",
    "    video_output_folder,\n",
    "    video_output_folder\n",
    ")\n",
    "\n",
    "move_videos_by_correction(\n",
    "    video_output_folder,\n",
    "    './label_correction.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27629142",
   "metadata": {},
   "source": [
    "#### Manual Label Correction ‚úçÔ∏è\n",
    "\n",
    "While K-Means provides an excellent starting point, automated clustering may not perfectly capture the nuances of human emotion in every case. To ensure the highest quality training data, a manual review and correction step is performed.\n",
    "\n",
    "A `label_correction.csv` file is used to document any necessary changes. This file specifies which videos should be moved from their `initial_label` (assigned by the clustering algorithm) to a more accurate `corrected_label` based on visual inspection. Below is an example of how this file is structured:\n",
    "\n",
    "| file_name | initial_label | corrected_label |\n",
    "|:----------|:--------------|:----------------|\n",
    "| `021.mp4` | Anger         | Shock           |\n",
    "| `113.mp4` | Anger         | Shock           |\n",
    "| `120.mp4` | Anger         | Sad             |\n",
    "\n",
    "The final step in our labeling process involves running a script that reads this CSV and moves the corresponding video files between folders, solidifying our ground-truth dataset before feature extraction begins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7daed89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_counts, total_frame_counts = get_frame_counts(video_output_folder, emotion_labels)\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 14))\n",
    "\n",
    "# KDE plot for frame count distribution\n",
    "for emotion, counts in frame_counts.items():\n",
    "    sns.kdeplot(counts, label=emotion, fill=True, alpha=0.3, linewidth=2, ax=axes[0])\n",
    "axes[0].axvline(x=15, color='red', linestyle='--', linewidth=2, label='15 Frames Threshold')\n",
    "axes[0].set_xlabel('Number of Frames per Video', fontsize=14)\n",
    "axes[0].set_ylabel('Density', fontsize=14)\n",
    "axes[0].set_title('Distribution of Frame Counts per Video for Each Emotion', fontsize=18, fontweight='bold')\n",
    "axes[0].legend(fontsize=12)\n",
    "axes[0].grid(True, linestyle='--', alpha=0.5)\n",
    "\n",
    "# Bar plot for total frame counts per emotion\n",
    "emotions = list(total_frame_counts.keys())\n",
    "totals = [total_frame_counts[emotion] for emotion in emotions]\n",
    "sns.barplot(x=totals, y=emotions, orient='h', palette='viridis', ax=axes[1])\n",
    "axes[1].set_xlabel('Total Number of Frames', fontsize=14)\n",
    "axes[1].set_ylabel('Emotion', fontsize=14)\n",
    "axes[1].set_title('Total Frame Counts per Emotion', fontsize=16, fontweight='bold')\n",
    "axes[1].grid(True, axis='x', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04159d66",
   "metadata": {},
   "source": [
    "#### üìä Analyzing Data Distribution and Class Balance\n",
    "\n",
    "Before building the model, it's essential to understand the characteristics of our processed dataset. The plots below examine the distribution of video lengths and the balance between our five emotion classes.\n",
    "\n",
    "**Distribution of Video Lengths**\n",
    "\n",
    "The top plot shows the distribution of video lengths (in frames) for each of the five emotion categories. We can see that most videos are between 25 and 75 frames long.\n",
    "\n",
    "The red dashed line marks the **15 Frames Threshold**. This is a critical parameter because our CNN-LSTM model is designed to process sequences of a fixed length (15 frames). This threshold was chosen as a strategic balance:\n",
    "* It is short enough that the vast majority of videos in the dataset can provide at least one complete 15-frame data sample, maximizing the amount of usable data.\n",
    "* It is long enough to capture meaningful temporal changes in a facial expression, which is crucial for the LSTM component of our model.\n",
    "\n",
    "**Total Frame Counts and Class Imbalance**\n",
    "\n",
    "The bottom bar chart reveals the total number of frames available for each emotion class. This plot highlights a very important characteristic of the dataset: **class imbalance**.\n",
    "\n",
    "As shown, the 'Neutral' class has significantly more data (over 12,000 frames) than any other class, while 'Anger' has the least (around 3,000 frames). This imbalance is a key challenge that must be considered during the modeling phase, as it can cause the model to become biased towards predicting the majority classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbac21e5",
   "metadata": {},
   "source": [
    "### üìú Stage 3: Face Mesh Extraction to Sequential CSVs\n",
    "\n",
    "This final preprocessing stage converts the labeled videos into a numerical format suitable for our CNN-LSTM model. For each video, this script:\n",
    "1.  **Extracts Landmarks**: Uses Google's MediaPipe Face Mesh to detect and extract the 3D coordinates (x, y, z) for 468 facial landmarks from every frame. To handle frames where a face is not detected, the landmarks from the previously successful frame are repeated.\n",
    "2.  **Chunks Data**: To prepare the data for a sequence model, the landmark data is chunked into sequences of **15 frames**.\n",
    "3.  **Exports to CSV**: Each 15-frame chunk is saved as a separate `.csv` file. If the last chunk of a video has between 10 and 14 frames, it is padded to 15 frames to ensure sequence consistency. This results in a dataset of thousands of CSV files, each representing a short, sequential facial expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537d351c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_video_to_csv(video_path, csv_dir):\n",
    "    \"\"\"\n",
    "    Extract face mesh landmarks from video and export to CSV files in chunks.\n",
    "    Each CSV contains up to 15 frames (padded if last chunk < 15 but >= 10).\n",
    "    \"\"\"\n",
    "    mp_face_mesh = mp.solutions.face_mesh\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    if total_frames < 10:\n",
    "        print(f\"Video {video_path} has less than 10 frames: {total_frames}\")\n",
    "\n",
    "    frame_idx = 0\n",
    "    chunk_idx = 1\n",
    "    chunk = []\n",
    "    last_row = None\n",
    "\n",
    "    # Prepare CSV header\n",
    "    header = ['frame']\n",
    "    for i in range(468):\n",
    "        header.extend([f'x_{i}', f'y_{i}', f'z_{i}'])\n",
    "\n",
    "    base_filename = os.path.splitext(os.path.basename(video_path))[0]\n",
    "\n",
    "    with mp_face_mesh.FaceMesh(\n",
    "        static_image_mode=False,\n",
    "        max_num_faces=1,\n",
    "        refine_landmarks=False,\n",
    "        min_detection_confidence=0.35,\n",
    "        min_tracking_confidence=0.35\n",
    "    ) as face_mesh:\n",
    "\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            results = face_mesh.process(rgb_frame)\n",
    "\n",
    "            if results.multi_face_landmarks:\n",
    "                for face_landmarks in results.multi_face_landmarks:\n",
    "                    row = [frame_idx]\n",
    "                    for landmark in face_landmarks.landmark:\n",
    "                        row.extend([landmark.x, landmark.y, landmark.z])\n",
    "                    last_row = row\n",
    "            else:\n",
    "                # If no face detected, repeate last row\n",
    "                if last_row is not None:\n",
    "                    row = [frame_idx] + last_row[1:]\n",
    "                else: # Case for first frame with no face detected\n",
    "                    row = [frame_idx] + [0] * (468 * 3)\n",
    "\n",
    "            chunk.append(row)\n",
    "            frame_idx += 1\n",
    "\n",
    "            # Write chunk to CSV if it reaches 15 frames\n",
    "            if len(chunk) == 15:\n",
    "                csv_filename = f\"{base_filename}_{chunk_idx:03d}.csv\"\n",
    "                csv_path = os.path.join(csv_dir, csv_filename)\n",
    "                with open(csv_path, 'w', newline='') as csv_file:\n",
    "                    writer = csv.writer(csv_file)\n",
    "                    writer.writerow(header)\n",
    "                    writer.writerows(chunk)\n",
    "                chunk_idx += 1\n",
    "                chunk = []\n",
    "\n",
    "        # If last chunk has 10 <= frame < 15, pad to 15 and save\n",
    "        if 10 <= len(chunk) < 15:\n",
    "            while len(chunk) < 15:\n",
    "                chunk.append(chunk[-1][:])\n",
    "            csv_filename = f\"{base_filename}_{chunk_idx:03d}.csv\"\n",
    "            csv_path = os.path.join(csv_dir, csv_filename)\n",
    "            with open(csv_path, 'w', newline='') as csv_file:\n",
    "                writer = csv.writer(csv_file)\n",
    "                writer.writerow(header)\n",
    "                writer.writerows(chunk)\n",
    "\n",
    "    cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fb20f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video_to_csv(args):\n",
    "    emotion, video_file = args\n",
    "    \n",
    "    emotion_folder = os.path.join(video_output_folder, emotion)\n",
    "    facemesh_emotion_folder = os.path.join(facemesh_output_folder, emotion)\n",
    "    \n",
    "    os.makedirs(facemesh_emotion_folder, exist_ok=True)\n",
    "    video_path = os.path.join(emotion_folder, video_file)\n",
    "        \n",
    "    export_video_to_csv(video_path, facemesh_emotion_folder)\n",
    "\n",
    "# Collect all video tasks for processing\n",
    "video_tasks = []\n",
    "for emotion in os.listdir(video_output_folder):\n",
    "    emotion_folder = os.path.join(video_output_folder, emotion)\n",
    "\n",
    "    for video_file in os.listdir(emotion_folder):\n",
    "        video_tasks.append((emotion, video_file))\n",
    "            \n",
    "with ThreadPoolExecutor(max_workers=os.cpu_count() * 2) as executor:\n",
    "    list(tqdm.tqdm(executor.map(process_video_to_csv, video_tasks), total=len(video_tasks), desc='Exporting FaceMesh CSV'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e6b0fa",
   "metadata": {},
   "source": [
    "## üß† 3. Modeling\n",
    "\n",
    "With the data processed and structured into sequential CSV files, we can now build and train our deep learning model. This section covers the entire modeling pipeline using **PyTorch**, from loading the data to evaluating the final model's performance.\n",
    "\n",
    "The modeling process is broken down into the following key steps:\n",
    "\n",
    "1.  **Data Loading & Preparation**:\n",
    "    * A custom PyTorch `Dataset` class (`FacemeshDataset`) is created to efficiently load the thousands of CSV files in parallel.\n",
    "    * The dataset is split into a **training set** and a **validation set**. A `stratified` split is used to ensure that the class imbalance observed during our data analysis is preserved in both sets, leading to more reliable validation.\n",
    "    * PyTorch `DataLoaders` are used to handle batching and shuffling of the data during training.\n",
    "\n",
    "2.  **Model Architecture: The CNN-LSTM**:\n",
    "    A hybrid **Convolutional Neural Network (CNN) and Long Short-Term Memory (LSTM)** model is designed to effectively learn from the spatio-temporal facial landmark data.\n",
    "    * üß† **1D CNN Layers**: The model first uses 1D convolutional layers to act as a powerful feature extractor. These layers identify complex **spatial patterns** among the 468 facial landmarks within each time step of the 15-frame sequence.\n",
    "    * üß† **Bidirectional LSTM Layers**: The features extracted by the CNN are then fed into a bidirectional LSTM. The LSTM's strength is in learning **temporal patterns**‚Äîit analyzes how the facial landmarks evolve over the 15-frame sequence. Using a *bidirectional* LSTM allows the model to learn from the sequence in both forward and reverse directions, capturing a more complete context of the expression.\n",
    "    * üß† **Classifier Head**: Finally, a set of fully connected layers act as the classifier, taking the learned features from the LSTM and making the final prediction among the five emotion classes.\n",
    "\n",
    "3.  **Training & Validation**:\n",
    "    * The model is trained for 25 epochs using the **Adam optimizer** and **Cross-Entropy Loss** function, which are standard choices for multi-class classification.\n",
    "    * After each epoch of training, the model's performance is evaluated on the unseen validation set.\n",
    "    * The model weights are saved after every epoch, and the best-performing model (based on the highest validation accuracy) is specifically saved for final evaluation.\n",
    "\n",
    "4.  **Evaluation**:\n",
    "    * After training is complete, the best saved model is loaded.\n",
    "    * Its performance is thoroughly assessed using multiple metrics: overall accuracy, a detailed **classification report** (with precision, recall, and F1-score for each emotion), and a **confusion matrix** to visualize which emotions the model confuses with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3c7365",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import time\n",
    "import csv\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cf213c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FacemeshDataset(Dataset):\n",
    "    def __init__(self, file_paths, labels, label_map):\n",
    "        super().__init__()\n",
    "        self.file_paths = file_paths\n",
    "        self.str_labels = labels\n",
    "        self.label_map = label_map\n",
    "        \n",
    "        self.data_tensor = []\n",
    "        self.label_tensor = []\n",
    "        \n",
    "        self.idx_to_label = {v: k for k, v in label_map.items()}\n",
    "\n",
    "        # Load CSV to Tensor\n",
    "        def _load_file(args):\n",
    "            file_path, label_str = args\n",
    "            try:\n",
    "                df = pd.read_csv(file_path)\n",
    "                df = df.drop(columns=['frame'], errors='ignore')\n",
    "                \n",
    "                data = torch.tensor(df.values, dtype=torch.float32)\n",
    "                label_int = self.label_map[label_str]\n",
    "                label = torch.tensor(label_int, dtype=torch.long)\n",
    "                return data, label\n",
    "            except Exception as e:\n",
    "                print(f\"Skipping file due to error: {file_path} | Error: {e}\")\n",
    "                return None, None\n",
    "\n",
    "        # Parallelized Data Loading\n",
    "        tasks = zip(self.file_paths, self.str_labels)\n",
    "        with ThreadPoolExecutor(max_workers=os.cpu_count() * 2) as executor:\n",
    "            results = list(tqdm(executor.map(_load_file, tasks), total=len(self.file_paths), desc=\"Processing files\"))\n",
    "\n",
    "        for data, label in results:\n",
    "            if data is not None and label is not None:\n",
    "                self.data_tensor.append(data)\n",
    "                self.label_tensor.append(label)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_tensor)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data_tensor[idx], self.label_tensor[idx]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21796a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './Data/AFEW-VA/FaceMesh'\n",
    "batch_size = 48 # Change batch size according to VRAM\n",
    "valid_split = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e453b4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather all csv paths and its label\n",
    "all_files = []\n",
    "all_labels_str = []\n",
    "emotion_folders = [d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))]\n",
    "for emotion in emotion_folders:\n",
    "    emotion_path = os.path.join(data_dir, emotion)\n",
    "    csv_files = [f for f in os.listdir(emotion_path) if f.endswith('.csv')]\n",
    "    for csv_file in csv_files:\n",
    "        full_path = os.path.join(emotion_path, csv_file)\n",
    "        all_files.append(full_path)\n",
    "        all_labels_str.append(emotion)\n",
    "\n",
    "unique_labels = sorted(list(set(all_labels_str)))\n",
    "label_map = {label: i for i, label in enumerate(unique_labels)}\n",
    "\n",
    "# Randomly split train & val\n",
    "train_files, val_files, train_labels_str, val_labels_str = train_test_split(\n",
    "    all_files, all_labels_str, \n",
    "    test_size=valid_split, \n",
    "    random_state=42, \n",
    "    stratify=all_labels_str\n",
    ")\n",
    "\n",
    "# Load dataset\n",
    "train_dataset = FacemeshDataset(train_files, train_labels_str, label_map)\n",
    "val_dataset = FacemeshDataset(val_files, val_labels_str, label_map)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6729503",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 1404 # Facemesh have 1404 point\n",
    "num_classes = len(label_map)\n",
    "\n",
    "# Train parameters\n",
    "learning_rate = 0.0001\n",
    "epochs = 2500 # Adjust if if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3159fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotiMesh_Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, \n",
    "                 cnn_out_channels, kernel_size, stride, padding,\n",
    "                 lstm_dropout, classifier_dropout):\n",
    "        super(EmotiMesh_Net, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.input_bn = nn.BatchNorm1d(input_size)\n",
    "\n",
    "        # 1D Conv block\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, \n",
    "                               out_channels=cnn_out_channels, \n",
    "                               kernel_size=kernel_size,\n",
    "                               stride=stride,\n",
    "                               padding=padding)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Calculate output size from CNN\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, 1, input_size)\n",
    "            cnn_out_length = self.pool1(self.conv1(dummy_input)).shape[2]\n",
    "        \n",
    "        self.cnn_output_size = cnn_out_channels * cnn_out_length\n",
    "\n",
    "        # Bi-LSTM block\n",
    "        self.lstm = nn.LSTM(input_size=self.cnn_output_size, \n",
    "                            hidden_size=hidden_size, \n",
    "                            num_layers=num_layers, \n",
    "                            batch_first=True, \n",
    "                            bidirectional=True,\n",
    "                            dropout=lstm_dropout if num_layers > 1 else 0)\n",
    "        \n",
    "        # Classifier head\n",
    "        self.fc1 = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.bn_fc1 = nn.BatchNorm1d(hidden_size)\n",
    "        self.relu_fc1 = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(classifier_dropout)\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_length, _ = x.shape\n",
    "        x_reshaped = x.view(batch_size * seq_length, -1)\n",
    "        x_bn = self.input_bn(x_reshaped)\n",
    "        \n",
    "        x_cnn_input = x_bn.view(batch_size * seq_length, 1, -1)\n",
    "        cnn_out = self.pool1(self.relu1(self.conv1(x_cnn_input)))\n",
    "        \n",
    "        lstm_input = cnn_out.view(batch_size, seq_length, -1)\n",
    "        h0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        lstm_out, _ = self.lstm(lstm_input, (h0, c0)) \n",
    "        last_time_step_output = lstm_out[:, -1, :]\n",
    "        \n",
    "        out = self.dropout(self.relu_fc1(self.bn_fc1(self.fc1(last_time_step_output))))\n",
    "        out = self.fc2(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f47a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EmotiMesh_Net(\n",
    "    input_size=input_size,\n",
    "    hidden_size=192,\n",
    "    num_layers=2,\n",
    "    num_classes=num_classes, # Make sure this is defined\n",
    "    cnn_out_channels=128,\n",
    "    kernel_size=7,\n",
    "    stride=1,\n",
    "    padding=2,\n",
    "    lstm_dropout=0.25,\n",
    "    classifier_dropout=0.25\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.999)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c25388e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup for saving model and logging\n",
    "SAVE_DIR = \"./Models\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "best_val_loss = float('inf')\n",
    "best_val_acc = 0.0\n",
    "\n",
    "log_file_path = os.path.join(SAVE_DIR, 'model_logs.csv')\n",
    "log_header = ['epoch', 'time_seconds', 'train_acc', 'train_loss', 'val_acc', 'val_loss', 'learning_rate']\n",
    "with open(log_file_path, 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(log_header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09698453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "start_time = time.time()\n",
    "for epoch in range(epochs):\n",
    "    # Training Phase\n",
    "    model.train()\n",
    "    running_loss_train = 0.0\n",
    "    correct_predictions_train = 0\n",
    "    \n",
    "    for sequences, labels in train_loader:\n",
    "        sequences, labels = sequences.to(device), labels.to(device)\n",
    "        \n",
    "        outputs = model(sequences)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate train metrics\n",
    "        running_loss_train += loss.item() * sequences.size(0)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct_predictions_train += (predicted == labels).sum().item()\n",
    "\n",
    "    epoch_loss_train = running_loss_train / len(train_loader.dataset)\n",
    "    epoch_acc_train = correct_predictions_train / len(train_loader.dataset)\n",
    "\n",
    "    # Validation Phase\n",
    "    model.eval()\n",
    "    running_loss_val = 0.0\n",
    "    correct_predictions_val = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for sequences, labels in val_loader:\n",
    "            sequences, labels = sequences.to(device), labels.to(device)\n",
    "            outputs = model(sequences)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Accumulate validation metrics\n",
    "            running_loss_val += loss.item() * sequences.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct_predictions_val += (predicted == labels).sum().item()\n",
    "\n",
    "    epoch_loss_val = running_loss_val / len(val_loader.dataset)\n",
    "    epoch_acc_val = correct_predictions_val / len(val_loader.dataset)\n",
    "    \n",
    "    current_lr = scheduler.get_last_lr()[0]\n",
    "    \n",
    "    # Print Epoch Results\n",
    "    print(f\"Epoch {epoch+1}/{epochs} | \"\n",
    "          f\"Train Loss: {epoch_loss_train:.4f}, Train Acc: {epoch_acc_train:.4f} | \"\n",
    "          f\"Val Loss: {epoch_loss_val:.4f}, Val Acc: {epoch_acc_val:.4f} | \"\n",
    "          f\"LR: {current_lr:.6f}\") \n",
    "    \n",
    "    \n",
    "    # Save the last model\n",
    "    torch.save(model.state_dict(), os.path.join(SAVE_DIR, 'last_model.pth'))\n",
    "\n",
    "    # Save the model with the best validation loss\n",
    "    if epoch_loss_val < best_val_loss:\n",
    "        best_val_loss = epoch_loss_val\n",
    "        torch.save(model.state_dict(), os.path.join(SAVE_DIR, 'best_val_loss_model.pth'))\n",
    "        print(f\"‚úÖ New best validation loss model saved: {best_val_loss:.4f}\")\n",
    "\n",
    "    # Save the model with the best validation accuracy\n",
    "    if epoch_acc_val > best_val_acc:\n",
    "        best_val_acc = epoch_acc_val\n",
    "        torch.save(model.state_dict(), os.path.join(SAVE_DIR, 'best_val_acc_model.pth'))\n",
    "        print(f\"‚úÖ New best validation accuracy model saved: {best_val_acc:.4f}\")\n",
    "        \n",
    "    print('-' * 50)\n",
    "    \n",
    "    # Logging to csv\n",
    "    elapsed_time = time.time() - start_time\n",
    "    log_data = [epoch + 1, elapsed_time, epoch_acc_train, epoch_loss_train, epoch_acc_val, epoch_loss_val, current_lr]\n",
    "    with open(log_file_path, 'a', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(log_data)\n",
    "    \n",
    "    scheduler.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb15603",
   "metadata": {},
   "source": [
    "## üìà 4. Model Evaluation & Results\n",
    "\n",
    "After training, the best-performing model is loaded and evaluated on the unseen validation set. The model achieves a strong **overall accuracy of 79.74%**, indicating that it can correctly classify the emotion in videos approximately 4 out of 5 times.\n",
    "\n",
    "A more detailed breakdown of its performance on each emotion class reveals specific strengths and areas for improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755acd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "best_model_path = os.path.join('./Models', 'best_val_loss_model.pth')\n",
    "\n",
    "# Load architecture\n",
    "model = EmotiMesh_Net(\n",
    "    input_size=input_size,\n",
    "    hidden_size=192,\n",
    "    num_layers=2,\n",
    "    num_classes=num_classes,\n",
    "    cnn_out_channels=128,\n",
    "    kernel_size=7,\n",
    "    stride=1,\n",
    "    padding=2,\n",
    "    lstm_dropout=0.25,\n",
    "    classifier_dropout=0.25\n",
    ")\n",
    "# Load weight\n",
    "model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdddaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "# Evaluate model on val dataset\n",
    "with torch.no_grad():\n",
    "    for sequences, labels in tqdm(val_loader, desc=\"Evaluating Model\"):\n",
    "        sequences, labels = sequences.to(device), labels.to(device)\n",
    "        \n",
    "        outputs = model(sequences)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5c6c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "class_names = [val_dataset.idx_to_label[i] for i in range(num_classes)]\n",
    "report = classification_report(all_labels, all_preds, target_names=class_names)\n",
    "\n",
    "print(f\"Overall Model Accuracy: {(round(accuracy, 4))*100}%\")\n",
    "print(\"Classification Report:\\n\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adee5668",
   "metadata": {},
   "source": [
    "After training, the best-performing model is loaded and evaluated on the unseen validation set. The model achieves a strong **overall accuracy of 79.74%**, indicating that it can correctly classify the emotion in videos approximately 4 out of 5 times.\n",
    "\n",
    "A more detailed breakdown of its performance on each emotion class reveals specific strengths and areas for improvement.\n",
    "\n",
    "### ‚úÖ Strengths\n",
    "\n",
    "* **Excellent Recall for 'Neutral'**: The model is exceptionally good at identifying `Neutral` expressions, correctly finding 93% of all neutral samples in the dataset. This high recall is likely supported by 'Neutral' being the majority class with 149 samples.\n",
    "* **High Precision for 'Sad'**: When the model predicts an emotion is `Sad`, it is highly reliable, being correct 92% of the time. This suggests a low rate of false positives for this class.\n",
    "* **Robust Performance on Minority Class**: Despite `Anger` having the fewest samples in the validation set (support: 40), the model performs well on it, achieving a balanced F1-score of 0.79.\n",
    "\n",
    "### ‚ö†Ô∏è Weaknesses & Areas for Improvement\n",
    "\n",
    "* **Difficulty Recognizing 'Happy'**: The model's most significant weakness is in identifying `Happy` expressions. It only manages to recall 67% of the true happy samples, meaning it misses about one-third of them. These may be misclassified as other emotions like 'Neutral' or 'Shock'.\n",
    "* **Lower Performance on 'Shock'**: The 'Shock' category also shows room for improvement, with a lower F1-score of 0.74 compared to other classes like 'Neutral' (0.84) and 'Sad' (0.82).\n",
    "* **Influence of Class Imbalance**: The model's strong performance on the `Neutral` class might be partially influenced by it having significantly more training data than any other emotion.\n",
    "\n",
    "In summary, the model shows strong promise with good overall performance. The primary area for future work would be to improve the recognition of `Happy` expressions, possibly by gathering more diverse training examples for that class or by fine-tuning the model further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24a3407",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file_path = './Models/model_logs.csv'\n",
    "logs_df = pd.read_csv(log_file_path)\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.style.use('seaborn-v0_8-deep')\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10), sharex=True)\n",
    "\n",
    "# Val Accuracy\n",
    "ax1.plot(logs_df['epoch'], logs_df['train_acc'], label='Training Accuracy', linewidth=0.75)\n",
    "ax1.plot(logs_df['epoch'], logs_df['val_acc'], label='Validation Accuracy', linewidth=0.75)\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.set_title('Model Accuracy vs. Epochs')\n",
    "\n",
    "# Find best val accuracy\n",
    "best_val_acc_epoch = logs_df['val_acc'].idxmax()\n",
    "best_val_acc = logs_df['val_acc'].max()\n",
    "ax1.axvline(x=logs_df['epoch'][best_val_acc_epoch], color='g', linestyle='--', label=f'Best Val Acc: {best_val_acc:.4f} at Epoch {int(logs_df[\"epoch\"][best_val_acc_epoch])}')\n",
    "ax1.legend()\n",
    "\n",
    "\n",
    "# Val Loss\n",
    "ax2.plot(logs_df['epoch'], logs_df['train_loss'], label='Training Loss', linewidth=0.75)\n",
    "ax2.plot(logs_df['epoch'], logs_df['val_loss'], label='Validation Loss', linewidth=0.75)\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Loss')\n",
    "ax2.set_title('Model Loss vs. Epochs')\n",
    "\n",
    "# Find best val loss\n",
    "best_val_loss_epoch = logs_df['val_loss'].idxmin()\n",
    "best_val_loss = logs_df['val_loss'].min()\n",
    "ax2.axvline(x=logs_df['epoch'][best_val_loss_epoch], color='r', linestyle='--', label=f'Best Val Loss: {best_val_loss:.4f} at Epoch {int(logs_df[\"epoch\"][best_val_loss_epoch])}')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8948b94a",
   "metadata": {},
   "source": [
    "The plots above visualize the model's performance throughout the entire training process, comparing its accuracy and loss on the training data versus the unseen validation data for each epoch.\n",
    "\n",
    "* **Accuracy Analysis**: The top plot shows the model's accuracy. The training accuracy (blue line) steadily climbs to over 90%, indicating the model's ability to learn the training data effectively. The validation accuracy (green line) also increases steadily but begins to plateau around the **79.74%** mark, which it achieves at **epoch 1967**. The gap between these two lines is a classic sign of overfitting, where the model performs better on data it has already seen.\n",
    "\n",
    "* **Loss Analysis**: The bottom plot provides a clearer view of the overfitting. The training loss (blue line) continues to decrease for the entire duration, showing the model is still finding ways to better fit the training samples. However, the validation loss (green line) finds its minimum value of **0.7093 at epoch 1967** and then remains flat and noisy. This divergence means that after this point, further training did not improve the model's ability to generalize to new data.\n",
    "\n",
    "This analysis confirms that our strategy of saving the model based on the best validation score was crucial for selecting the most effective and least overfit version of the model for our final evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e002a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f255a9",
   "metadata": {},
   "source": [
    "* üîé **Visual Deep-Dive**: The confusion matrix offers a detailed analysis of the model's predictions, showing exactly where it succeeds and where it makes errors.\n",
    "\n",
    "* üß† **The 'Neutral' Gravity Well**: The model's most notable pattern is a strong tendency to misclassify other emotions as `Neutral`. This is its most common type of error, likely influenced by the class imbalance in the dataset.\n",
    "\n",
    "* üìâ **Key Misclassifications**: The most significant errors involve videos being mislabeled as `Neutral`, including 13 `Happy` instances and 14 `Shock` instances.\n",
    "\n",
    "* ‚ÜîÔ∏è **High-Arousal Confusion**: The model sometimes confuses emotions with similar high-energy expressions, for example, misclassifying 4 `Anger` videos as `Shock` and 4 `Shock` videos as `Anger`.\n",
    "\n",
    "* ‚úÖ **Overall Insight**: The matrix confirms that while the model is generally effective, its primary challenges are overcoming the bias towards the majority `Neutral` class and better distinguishing between emotions with overlapping high-arousal characteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5d2a6a",
   "metadata": {},
   "source": [
    "## üèÅ Project Conclusion & Future Work\n",
    "\n",
    "This project successfully developed a complete, end-to-end pipeline for video-based emotion recognition. Starting from a dataset with continuous emotional scores, we engineered a novel labeling process, extracted advanced spatial-temporal features, and trained a robust deep learning model, achieving a strong final performance. The entire workflow demonstrates a viable and effective approach to tackling complex, real-world emotion classification tasks.\n",
    "\n",
    "### ‚ú® Key Achievements\n",
    "\n",
    "* **Novel Data Processing Pipeline**: Successfully created a workflow to address the primary challenge of the AFEW-VA dataset, which lacks discrete emotion labels. This was achieved by:\n",
    "    * Averaging the raw `valence` and `arousal` scores for each video.\n",
    "    * Applying K-Means clustering to group videos into natural emotional categories.\n",
    "    * Performing manual curation to refine the labels, resulting in five distinct classes: `Anger`, `Happy`, `Shock`, `Neutral`, and `Sad`.\n",
    "\n",
    "* **Hybrid CNN-LSTM Model**: Implemented a powerful `EmotiMesh-Net` architecture in PyTorch.\n",
    "    * The model leverages **MediaPipe** to extract 468 3D facial landmarks as input features.\n",
    "    * A **1D-CNN** first extracts spatial patterns from the landmark data within each frame.\n",
    "    * A **bidirectional LSTM** then analyzes the temporal evolution of these patterns across 15-frame sequences.\n",
    "\n",
    "* **Strong Final Performance**: The trained model achieved a final **validation accuracy of 79.74%**, proving the effectiveness of the overall approach.\n",
    "\n",
    "### üß† Key Learnings & Insights\n",
    "\n",
    "* **Model Strengths**: The evaluation revealed that the model is particularly effective at recognizing certain emotions.\n",
    "    * It has excellent recall for the `Neutral` class, correctly identifying 93% of all neutral samples.\n",
    "    * It is highly precise when predicting `Sad`, being correct 92% of the time.\n",
    "\n",
    "* **Model Weaknesses**: The analysis also highlighted clear areas for improvement.\n",
    "    * The model's biggest challenge is recognizing `Happy` expressions, failing to identify about one-third of the happy samples (67% recall).\n",
    "    * The confusion matrix shows a strong bias towards the majority `Neutral` class, where other emotions are frequently misclassified. This is a direct result of the **class imbalance** in the dataset.\n",
    "\n",
    "### üöÄ Future Work\n",
    "\n",
    "Based on the results, the following steps could be taken to further improve the model:\n",
    "\n",
    "* **Address Class Imbalance**: Implement techniques like oversampling the minority classes (e.g., `Anger`, `Sad`) or using a weighted loss function during training to give more importance to under-represented emotions.\n",
    "* **Improve 'Happy' Recognition**: Augment the dataset with more diverse examples of `Happy` expressions or explore feature engineering techniques that better capture the subtle cues of a genuine smile.\n",
    "* **Architectural Enhancements**: Experiment with adding an **Attention mechanism** to the LSTM. This could help the model focus on the most emotionally expressive frames within a 15-frame sequence, potentially improving accuracy on nuanced expressions.\n",
    "* **Deployment**: Package the final model into an API and integrate it into the front-end of the conceptual journaling application to complete the project's vision."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch12.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
